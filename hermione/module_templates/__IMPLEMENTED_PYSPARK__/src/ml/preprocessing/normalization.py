from pyspark.ml.base import Estimator
from pyspark.sql.dataframe import DataFrame
from pyspark.ml.feature import (
    VectorAssembler,
    MinMaxScaler,
    MaxAbsScaler,
    StandardScaler,
    RobustScaler,
)
from pyspark.ml.util import MLWriter, MLReader
from pyspark.ml.pipeline import Pipeline

class SparkScaler(Estimator, MLWriter, MLReader):

    def __init__(self, cols, method, vec_name = None):
        """ 
        Constructor
        
    	Parameters
    	----------            
        cols         : Union[list, str]
            columns to be normalized
        method         : str
            normalization method. Possible values are `min_max`, `max_abs`, `zscore`, `robust`.
        vec_name         : str
            optional name of the column generated by the `VectorAssembler`.
                     
    	Returns
    	-------
        SparkScaler
        """
        super(SparkScaler, self).__init__()
        self.cols = cols if type(cols) is list else [cols]
        self.vec_name = method + '_vec' if not vec_name else vec_name
        norm_methods = {
            'min_max': MinMaxScaler,
            'max_abs': MaxAbsScaler,
            'zscore': StandardScaler,
            'robust': RobustScaler,
        }
        if method not in norm_methods.keys():
            raise Exception('Method not supported. Choose one of the available normalization methods.')
        self.method_name = method
        self.scale_method = norm_methods[method]
    
    def _fit(self):
        """
        Implements abstract method
        
    	Parameters
    	----------            
    	Returns
    	-------
        SparkScaler
        """
        super()._fit()

    def create_vector(self):
        """
        Creates the vector assembler used to prepare the input cols
        
    	Parameters
    	----------            
    	Returns
    	-------
        SparkScaler
        """
        self.assembler = VectorAssembler(
            inputCols=self.cols,
            outputCol=self.vec_name, 
            handleInvalid='skip'
        )

    def create_scaler(self, **kwargs):
        """
        Creates the model used in the actual normalization
        
    	Parameters
    	----------            
    	Returns
    	-------
        SparkScaler
        """
        self.scaler = (
            self.scale_method(**kwargs)
            .setInputCol(self.vec_name)
            .setOutputCol(f'{self.method_name}_scaled')
        )

    def fit(self, df, **kwargs):
        """
        Generates normalization object for each column
        
    	Parameters
    	----------            
        df         : pd.DataFrame
            dataframe with columns to be normalized             
        
        **kwargs:
            Other arguments passed to the Estimator

    	Returns
    	-------
        pyspark.ml.base.Transformer
            Transformer objected of the normalizer
        """
        self.create_vector()
        self.create_scaler(**kwargs)
        pipeline = Pipeline(stages=[self.assembler, self.scaler])
        self.model = pipeline.fit(df)
        return self.model

    def transform(self, df):
        """
        Apply normalization to the selected columns
        
    	Parameters
    	----------            
        df         : pyspark.sql.dataframe.DataFrame
            dataframe with columns to be normalized             
                     
    	Returns
    	-------
        pyspark.sql.dataframe.DataFrame
            Normalized dataframe
        """
        try:
            return self.model.transform(df)
        except:
            raise Exception('Model no fit! Run `fit()` method before trying to `transform()`')

    def fit_transform(self, df, **kwargs):
        """
        Generates the normalization model and apply it to the selected columns
        
    	Parameters
    	----------            
        df         : pyspark.sql.dataframe.DataFrame
            dataframe with columns to be normalized

        **kwargs:
            Other arguments passed to the Estimator      
                     
    	Returns
    	-------
        pyspark.sql.dataframe.DataFrame
            Normalized dataframe
        """
        self.create_vector()
        self.create_scaler(**kwargs)
        pipeline = Pipeline(stages=[self.assembler, self.scaler])
        self.model = pipeline.fit(df)
        return self.model.transform(df)
        
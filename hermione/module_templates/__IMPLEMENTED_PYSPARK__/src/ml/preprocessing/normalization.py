from ._base import CustomEstimator

from pyspark.ml.feature import (
    VectorAssembler,
    MinMaxScaler,
    MaxAbsScaler,
    StandardScaler,
    RobustScaler,
)
from pyspark.ml.pipeline import Pipeline

class SparkScaler(CustomEstimator):

    def __init__(self, cols, method, **kwargs):
        """ 
        Constructor
        
    	Parameters
    	----------            
        cols         : Union[list, str]
            columns to be normalized

        method         : str
            normalization method. Possible values are `min_max`, `max_abs`, `zscore`, `robust`.

        vec_name         : str
            optional name of the column generated by the `VectorAssembler`.

        **kwargs:
            Other arguments passed to the Scaling Estimator

    	Returns
    	-------
        SparkScaler
        """
        cols = cols if type(cols) is list else [cols]
        norm_methods = {
            'min_max': MinMaxScaler(),
            'max_abs': MaxAbsScaler,
            'zscore': StandardScaler,
            'robust': RobustScaler,
        }
        if method not in norm_methods.keys():
            options = '`' + '`, `'.join(norm_methods.keys()) + '`.'
            raise Exception(f'Method not supported. Choose one from {options}')
        assembler = (
            VectorAssembler(handleInvalid='skip')
            .setInputCols(cols)
            .setOutputCol(f'{method}_vec')
        )
        scaler = (
            norm_methods[method](**kwargs)
            .setInputCol(f'{method}_vec')
            .setOutputCol(f'{method}_scaled')
        )
        pipeline = Pipeline(stages=[assembler, scaler])
        super().__init__(pipeline)